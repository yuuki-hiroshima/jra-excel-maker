# v0.14 (æ”¹å–„ç‰ˆï¼šè¤‡æ•°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒä½µç”¨)
import os, re, threading
from datetime import datetime, timedelta
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext

import requests
from bs4 import BeautifulSoup
from bs4.element import Tag
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font, Border, Side, Alignment

# tkcalendar ã¯ä»»æ„ï¼ˆã‚ã‚‹å ´åˆã¯ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ä½¿ç”¨ï¼‰
TKCAL_OK = True
try:
    from tkcalendar import DateEntry, Calendar
except Exception:
    TKCAL_OK = False

VENUES = ["æœ­å¹Œ","å‡½é¤¨","ç¦å³¶","æ–°æ½Ÿ","æ±äº¬","ä¸­å±±","ä¸­äº¬","äº¬éƒ½","é˜ªç¥","å°å€‰"]
VENUE_CODE = {
    "æœ­å¹Œ":"01","å‡½é¤¨":"02","ç¦å³¶":"03","æ–°æ½Ÿ":"04",
    "æ±äº¬":"05","ä¸­å±±":"06","ä¸­äº¬":"07","äº¬éƒ½":"08",
    "é˜ªç¥":"09","å°å€‰":"10",
}

# ================== HTMLãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==================
def anchor_text(cell: Tag) -> str:
    if hasattr(cell, "find"):
        a = cell.find("a")
        if a:
            t = a.get_text(strip=True)
            if t:
                return t
        return cell.get_text(strip=True)
    return str(cell).strip()

def clean_name(s: str) -> str:
    s = re.split(r"[ï¼ˆ(]", s, maxsplit=1)[0]
    return s.strip()

def find_col_index(header_map: dict, candidates) -> int | None:
    for key in candidates:
        for h, idx in header_map.items():
            if h == key: return idx
    for key in candidates:
        for h, idx in header_map.items():
            if key in h: return idx
    return None

def find_table_and_headers(soup: BeautifulSoup):
    for t in soup.find_all("table"):
        thead = t.find("thead")
        head_cells = thead.find_all(["th","td"]) if thead else (t.find("tr").find_all(["th","td"]) if t.find("tr") else [])
        if not head_cells:
            continue
        heads_raw = [c.get_text(strip=True) for c in head_cells]
        heads_norm = [re.sub(r"\s+","", h) for h in heads_raw]
        has_horse = any("é¦¬å" in h for h in heads_norm)
        has_jock  = any(("é¨æ‰‹" in h) or ("é¨æ‰‹å" in h) for h in heads_norm)
        if has_horse and has_jock:
            return t, heads_raw, {h:i for i,h in enumerate(heads_norm)}
    return None, None, None

def extract_basic_meta(text_all: str):
    m_date = re.search(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", text_all)
    ymd = f"{int(m_date.group(1)):04d}{int(m_date.group(2)):02d}{int(m_date.group(3)):02d}" if m_date else datetime.now().strftime("%Y%m%d")
    m_place = re.search(r"\d+\s*å›\s*(æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰)\s*\d+\s*æ—¥", text_all)
    place = m_place.group(1) if m_place else "ä¸æ˜"
    m_r1 = re.search(r"(\d{1,2})\s*ãƒ¬ãƒ¼ã‚¹", text_all)
    m_r2 = re.search(r"(\d{1,2})\s*R", text_all)
    race_no = f"{int((m_r1 or m_r2).group(1))}R" if (m_r1 or m_r2) else "R"
    return ymd, place, race_no

# ================== URLæ¢ç´¢ï¼ˆè¤‡æ•°æˆ¦ç•¥ï¼‰ ==================
def try_fetch(url: str, debug_log=None):
    """URLã‹ã‚‰HTMLã‚’å–å¾—ã—ã€å‡ºé¦¬è¡¨ã¨ã—ã¦æœ‰åŠ¹ã‹åˆ¤å®š"""
    try:
        r = requests.get(url, headers={
            "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }, timeout=8)
        if debug_log:
            debug_log(f"è©¦è¡Œ: {url[:90]}... â†’ {r.status_code}")
    except Exception as e:
        if debug_log:
            debug_log(f"æ¥ç¶šå¤±æ•—: {str(e)[:50]}")
        return None
    
    if r.status_code != 200:
        return None
    
    r.encoding = r.apparent_encoding
    soup = BeautifulSoup(r.text, "lxml")
    
    # å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã®ç‰¹å¾´ã‚’ãƒã‚§ãƒƒã‚¯
    table, _, _ = find_table_and_headers(soup)
    if not table:
        return None
    
    # ãƒ¬ãƒ¼ã‚¹åè¦ç´ ã®å­˜åœ¨ç¢ºèª
    race_name = soup.select_one(".race_name") or soup.find(string=re.compile(r"\d+å›.*(æœ­å¹Œ|å‡½é¤¨|ç¦å³¶|æ–°æ½Ÿ|æ±äº¬|ä¸­å±±|ä¸­äº¬|äº¬éƒ½|é˜ªç¥|å°å€‰)"))
    if not race_name:
        return None
    
    if debug_log:
        debug_log(f"  âœ“ æœ‰åŠ¹ãªå‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‚’æ¤œå‡ºï¼")
    return soup

def strategy_1_pattern_analysis(yyyymmdd: str, place: str, race_no: int, debug_log=None):
    """
    æˆ¦ç•¥1: æä¾›ã•ã‚ŒãŸURLãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
    pw01dde01 05 20250501 11 20251108 /EB
            å ´æ‰€ é–‹å‚¬æ—¥   R# ä»Šæ—¥ã®æ—¥ä»˜
    
    ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®è¦å‰‡æ€§ã‚’æ¢ã‚‹
    """
    code = VENUE_CODE.get(place)
    if not code:
        return None, None
    
    if debug_log:
        debug_log("ã€æˆ¦ç•¥1ã€‘ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«ã‚ˆã‚‹æ¨æ¸¬")
    
    # ä»Šæ—¥ã®æ—¥ä»˜ã¨ãã®å‰å¾Œ
    today = datetime.now()
    access_dates = [
        today.strftime("%Y%m%d"),
        (today + timedelta(days=1)).strftime("%Y%m%d"),
        (today - timedelta(days=1)).strftime("%Y%m%d"),
    ]
    
    # è¦³æ¸¬ã•ã‚ŒãŸã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã¨ãã®å‘¨è¾º
    # EB=235, 39=57, 1B=27 (16é€²æ•°â†’10é€²æ•°)
    # ã“ã‚Œã‚‰ã‹ã‚‰è¦å‰‡æ€§ã‚’æ¨æ¸¬
    suffixes = ["EB", "39", "1B", "E9", "EA", "EC", "37", "38", "3A", "19", "1A", "1C", "1D"]
    
    race_variants = [f"{race_no:02d}", f"{race_no}"]
    endpoints = ["accessD.html", "accessS.html"]
    
    tried = 0
    for endpoint in endpoints:
        for rn in race_variants:
            for access_date in access_dates:
                for suffix in suffixes:
                    cname = f"pw01dde{code}{yyyymmdd}{rn}{access_date}/{suffix}"
                    url = f"https://www.jra.go.jp/JRADB/{endpoint}?CNAME={cname}"
                    tried += 1
                    
                    soup = try_fetch(url, debug_log)
                    if soup:
                        return url, soup
                    
                    if tried > 50:
                        return None, None
    
    return None, None

def strategy_2_date_variations(yyyymmdd: str, place: str, race_no: int, debug_log=None):
    """
    æˆ¦ç•¥2: æ—¥ä»˜ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ¢ç´¢
    é–‹å‚¬æ—¥ã¨å›æ•°ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
    """
    code = VENUE_CODE.get(place)
    if not code:
        return None, None
    
    if debug_log:
        debug_log("ã€æˆ¦ç•¥2ã€‘æ—¥ä»˜ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ¢ç´¢")
    
    # é–‹å‚¬æ—¥ã®å‰å¾Œã‚‚è©¦ã™ï¼ˆé€±æœ«é–‹å‚¬ãªã©ã®å¯èƒ½æ€§ï¼‰
    dt = datetime.strptime(yyyymmdd, "%Y%m%d")
    race_dates = [
        dt.strftime("%Y%m%d"),
        (dt - timedelta(days=1)).strftime("%Y%m%d"),
        (dt + timedelta(days=1)).strftime("%Y%m%d"),
    ]
    
    today = datetime.now().strftime("%Y%m%d")
    race_variants = [f"{race_no:02d}", f"{race_no}"]
    
    # ã‚ˆã‚Šåºƒç¯„å›²ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹
    for i in range(256):
        suffix = f"{i:02X}"
        for race_date in race_dates:
            for rn in race_variants:
                cname = f"pw01dde{code}{race_date}{rn}{today}/{suffix}"
                url = f"https://www.jra.go.jp/JRADB/accessD.html?CNAME={cname}"
                
                soup = try_fetch(url, debug_log)
                if soup:
                    return url, soup
                
                # 20ä»¶è©¦ã—ã¦è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°æ¬¡ã®æ—¥ä»˜ã¸
                if i > 20:
                    break
    
    return None, None

def strategy_3_scrape_pages(yyyymmdd: str, place: str, race_no: int, debug_log=None):
    """
    æˆ¦ç•¥3: JRAãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    """
    if debug_log:
        debug_log("ã€æˆ¦ç•¥3ã€‘JRAãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯æŠ½å‡º")
    
    urls_to_scrape = [
        f"https://www.jra.go.jp/keiba/thisweek/",
        f"https://www.jra.go.jp/keiba/thisweek/{yyyymmdd}/",
        f"https://www.jra.go.jp/",
    ]
    
    all_links = set()
    
    for page_url in urls_to_scrape:
        try:
            if debug_log:
                debug_log(f"ãƒšãƒ¼ã‚¸å–å¾—: {page_url}")
            
            r = requests.get(page_url, headers={
                "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }, timeout=10)
            
            if r.status_code != 200:
                continue
            
            r.encoding = r.apparent_encoding
            soup = BeautifulSoup(r.text, "lxml")
            
            # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã¨onClickã‚’æŠ½å‡º
            for elem in soup.find_all(["a", "button", "div", "span"]):
                # hrefå±æ€§
                href = elem.get("href", "")
                if "JRADB" in href and "CNAME" in href:
                    all_links.add(href if href.startswith("http") else f"https://www.jra.go.jp{href}")
                
                # onClickå±æ€§ã‹ã‚‰CNAMEæŠ½å‡º
                onclick = elem.get("onclick", "")
                match = re.search(r"CNAME=([^'\"&\s]+)", onclick)
                if match:
                    cname = match.group(1)
                    url = f"https://www.jra.go.jp/JRADB/accessD.html?CNAME={cname}"
                    all_links.add(url)
            
            if debug_log:
                debug_log(f"  {len(all_links)}å€‹ã®JRADBãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
        
        except Exception as e:
            if debug_log:
                debug_log(f"ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # æŠ½å‡ºã—ãŸãƒªãƒ³ã‚¯ã‚’è©¦è¡Œ
    code = VENUE_CODE.get(place)
    for url in all_links:
        # å ´æ‰€ã‚³ãƒ¼ãƒ‰ã¨ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’å«ã‚€URLã‚’å„ªå…ˆ
        if code in url or str(race_no) in url or f"{race_no:02d}" in url:
            if debug_log:
                debug_log(f"å€™è£œãƒªãƒ³ã‚¯ã‚’æ¤œè¨¼: {url[:80]}...")
            soup = try_fetch(url, debug_log)
            if soup:
                # æœ¬å½“ã«è©²å½“ãƒ¬ãƒ¼ã‚¹ã‹ç¢ºèª
                text = soup.get_text()
                if place in text and (f"{race_no}R" in text or f"{race_no}ãƒ¬ãƒ¼ã‚¹" in text):
                    return url, soup
    
    return None, None

def build_jra_url_and_soup(yyyymmdd: str, place: str, race_no: int, status_cb=None, debug_log=None):
    """è¤‡æ•°ã®æˆ¦ç•¥ã‚’é †æ¬¡è©¦è¡Œ"""
    
    strategies = [
        ("ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ", strategy_1_pattern_analysis),
        ("æ—¥ä»˜ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³", strategy_2_date_variations),
        ("ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°", strategy_3_scrape_pages),
    ]
    
    for name, strategy in strategies:
        if status_cb:
            status_cb(f"{name}ã§æ¢ç´¢ä¸­...")
        if debug_log:
            debug_log(f"\n{'='*60}")
            debug_log(f"æˆ¦ç•¥: {name}")
            debug_log(f"{'='*60}")
        
        try:
            url, soup = strategy(yyyymmdd, place, race_no, debug_log)
            if url and soup:
                if debug_log:
                    debug_log(f"\nâœ“ æˆåŠŸï¼ URLç™ºè¦‹: {url}")
                return url, soup
        except Exception as e:
            if debug_log:
                debug_log(f"æˆ¦ç•¥å¤±æ•—: {e}")
    
    return None, None

# ================== æŠ½å‡ºï¼†Excel ==================
def fetch_rows_and_meta(url: str, soup: BeautifulSoup | None = None):
    if soup is None:
        r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=15)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        soup = BeautifulSoup(r.text, "lxml")

    table, _, header_map = find_table_and_headers(soup)
    if not table:
        raise RuntimeError("å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆã€é¦¬åã€ã€é¨æ‰‹ã€ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    col_umaban = find_col_index(header_map, ["é¦¬ç•ª","é¦¬ç•ªå·"])
    col_horse  = find_col_index(header_map, ["é¦¬å"])
    col_jock   = find_col_index(header_map, ["é¨æ‰‹","é¨æ‰‹å"])
    if col_horse is None or col_jock is None:
        raise RuntimeError("ã€é¦¬åã€ã€é¨æ‰‹(é¨æ‰‹å)ã€åˆ—ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    trs = table.find_all("tr")
    start_idx = 1 if trs and trs[0].find_all("th") else 0

    rows = []
    for tr in trs[start_idx:]:
        tds = tr.find_all(["td","th"])
        if len(tds) <= max(col_horse, col_jock):
            continue

        horse  = clean_name(anchor_text(tds[col_horse]))
        jockey = clean_name(anchor_text(tds[col_jock]))
        if not horse or re.fullmatch(r"\d+", horse):
            continue
        if not jockey or jockey == "-":
            continue

        umaban = ""
        if col_umaban is not None and len(tds) > col_umaban:
            m = re.search(r"\d{1,2}", anchor_text(tds[col_umaban]).strip())
            umaban = m.group(0) if m else ""
        else:
            m = re.match(r"\D*(\d{1,2})\D*", anchor_text(tds[0]) if tds else "")
            umaban = m.group(1) if m else ""

        rows.append((umaban, horse, jockey))

    if not rows:
        raise RuntimeError("é¦¬ç•ªï¼é¦¬åï¼é¨æ‰‹åã®æŠ½å‡ºçµæœãŒç©ºã§ã—ãŸã€‚")

    race_el = soup.select_one(".race_name")
    race_title = race_el.get_text(strip=True).split("|")[0].strip() if race_el else ""

    text_all = soup.get_text(" ", strip=True)
    ymd, place, race_no = extract_basic_meta(text_all)
    if not race_title:
        race_title = f"{place}{race_no}"

    filename = f"{ymd}_{place}_{race_no}.xlsx"
    return rows, filename, race_title, url

def save_to_desktop(rows, filename, race_title):
    desktop = os.path.join(os.path.expanduser("~"), "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—")
    os.makedirs(desktop, exist_ok=True)
    path = os.path.join(desktop, filename)

    wb = Workbook()
    ws = wb.active
    ws.title = "å‡ºé¦¬è¡¨"

    ws.merge_cells("A1:E1")
    t = ws["A1"]; t.value = race_title
    t.alignment = Alignment(horizontal="center", vertical="center")
    t.font = Font(bold=True, size=18)
    t.fill = PatternFill(start_color="FADADD", end_color="FADADD", fill_type="solid")
    ws.row_dimensions[1].height = 30

    ws.append(["é¦¬ç•ª","é¦¬å","é¨æ‰‹å","è©•ä¾¡","çŸ­è©•"])
    for umaban, horse, jockey in rows:
        ws.append([umaban, horse, jockey, "", ""])

    light_blue = PatternFill(start_color="CCFFFF", end_color="CCFFFF", fill_type="solid")
    bold = Font(bold=True)
    thin = Side(style="thin", color="000000")
    border = Border(top=thin, bottom=thin, left=thin, right=thin)

    for c in range(1, 6):
        cell = ws.cell(row=2, column=c)
        cell.fill = light_blue
        cell.font = bold
        cell.alignment = Alignment(horizontal="center", vertical="center")

    for r in range(2, ws.max_row + 1):
        for c in range(1, 6):
            cell = ws.cell(row=r, column=c)
            cell.border = border
            cell.alignment = Alignment(horizontal="center", vertical="center")

    ws.column_dimensions["A"].width = 6
    ws.column_dimensions["B"].width = 28
    ws.column_dimensions["C"].width = 20
    ws.column_dimensions["D"].width = 10
    ws.column_dimensions["E"].width = 50

    wb.save(path)
    return path

# ================== GUI ==================
class App(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("JRAå‡ºé¦¬è¡¨ å–ã‚Šè¾¼ã¿ãƒ„ãƒ¼ãƒ«ï¼ˆ3æˆ¦ç•¥ä½µç”¨ç‰ˆï¼‰ - v0.14")
        self.geometry("750x550"); self.resizable(True, True)

        frm = ttk.Frame(self, padding=12); frm.pack(fill="both", expand=True)

        row = 0
        ttk.Label(frm, text="å–å¾—æ–¹æ³•").grid(row=row, column=0, sticky="w")
        self.mode = tk.StringVar(value="select")
        ttk.Radiobutton(frm, text="æ—¥ä»˜ãƒ»å ´æ‰€ãƒ»ãƒ¬ãƒ¼ã‚¹ã‚’é¸æŠ", variable=self.mode, value="select").grid(row=row, column=1, sticky="w")
        ttk.Radiobutton(frm, text="URLã‚’ç›´æ¥å…¥åŠ›",     variable=self.mode, value="url").grid(row=row, column=2, sticky="w")

        row += 1
        ttk.Label(frm, text="æ—¥ä»˜").grid(row=row, column=0, sticky="e", pady=6)
        if TKCAL_OK:
            self.date_widget = DateEntry(frm, width=14, date_pattern="yyyyMMdd")
        else:
            self.date_widget = ttk.Entry(frm, width=16)
            self.date_widget.insert(0, datetime.now().strftime("%Y%m%d"))
        self.date_widget.grid(row=row, column=1, sticky="w", pady=6)
        self.cal_btn = ttk.Button(frm, text="ğŸ“…", width=3, command=self.open_calendar); self.cal_btn.grid(row=row, column=2, sticky="w")

        ttk.Label(frm, text="å ´æ‰€").grid(row=row, column=3, sticky="e")
        self.cmb_place = ttk.Combobox(frm, values=VENUES, width=8, state="readonly"); self.cmb_place.grid(row=row, column=4, sticky="w"); self.cmb_place.set("äº¬éƒ½")

        row += 1
        ttk.Label(frm, text="ãƒ¬ãƒ¼ã‚¹").grid(row=row, column=0, sticky="e")
        self.cmb_race = ttk.Combobox(frm, values=[f"{i}R" for i in range(1,13)], width=6, state="readonly"); self.cmb_race.grid(row=row, column=1, sticky="w"); self.cmb_race.set("11R")

        row += 1
        ttk.Label(frm, text="URLï¼ˆä»»æ„ï¼šç›´å…¥åŠ›ã™ã‚‹å ´åˆï¼‰").grid(row=row, column=0, sticky="e")
        self.ent_url = ttk.Entry(frm, width=56); self.ent_url.grid(row=row, column=1, columnspan=4, sticky="ew")

        row += 1
        self.btn = ttk.Button(frm, text="Excelã‚’ä½œæˆï¼ˆãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã«ä¿å­˜ï¼‰", command=self.run_fetch); self.btn.grid(row=row, column=0, columnspan=5, sticky="ew", pady=12)

        row += 1
        self.status = ttk.Label(frm, text=f"å¾…æ©Ÿä¸­ / æ–¹å¼: 3æˆ¦ç•¥ä½µç”¨ / Calendar: {'ON' if TKCAL_OK else 'OFF'}"); self.status.grid(row=row, column=0, columnspan=5, sticky="w")

        row += 1
        ttk.Label(frm, text="ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚° (URLãŒè¦‹ã¤ã‹ã£ãŸã‚‰ã€æ¬¡å›ç”¨ã«ãƒ¡ãƒ¢ã—ã¦ãã ã•ã„):").grid(row=row, column=0, columnspan=5, sticky="w", pady=(10,0))
        
        row += 1
        self.debug_text = scrolledtext.ScrolledText(frm, height=12, width=80, state="disabled")
        self.debug_text.grid(row=row, column=0, columnspan=5, sticky="nsew", pady=5)

        frm.rowconfigure(row, weight=1)
        for i in range(5): frm.columnconfigure(i, weight=1)

    def debug_log(self, msg):
        """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã«è¿½è¨˜"""
        self.after(0, lambda: self._append_debug(msg))
    
    def _append_debug(self, msg):
        self.debug_text.config(state="normal")
        self.debug_text.insert(tk.END, msg + "\n")
        self.debug_text.see(tk.END)
        self.debug_text.config(state="disabled")

    def open_calendar(self):
        if not TKCAL_OK:
            top = tk.Toplevel(self); top.title("æ—¥ä»˜ã‚’å…¥åŠ›ï¼ˆYYYYMMDDï¼‰"); top.resizable(False, False)
            ttk.Label(top, text="tkcalendar ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\nYYYYMMDD å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚").pack(padx=10, pady=10)
            ent = ttk.Entry(top); ent.insert(0, self._current_ymd_or_today()); ent.pack(padx=10, pady=6)
            ttk.Button(top, text="ã“ã®æ—¥ä»˜ã§æ±ºå®š", command=lambda: (self.set_date_value(ent.get().strip()), top.destroy())).pack(pady=8)
            return
        cur = self._current_ymd_or_today(); y, m, d = int(cur[:4]), int(cur[4:6]), int(cur[6:8])
        top = tk.Toplevel(self); top.title("æ—¥ä»˜ã‚’é¸æŠ"); top.resizable(False, False)
        cal = Calendar(top, year=y, month=m, day=d, date_pattern="yyyy-mm-dd"); cal.pack(padx=10, pady=10)
        ttk.Button(top, text="ã“ã®æ—¥ä»˜ã§æ±ºå®š", command=lambda: (self.set_date_value(cal.get_date().replace("-", "")), top.destroy())).pack(pady=8)

    def _current_ymd_or_today(self) -> str:
        try:
            cur = self.date_widget.get().strip()
            if re.fullmatch(r"\d{8}", cur): return cur
        except Exception:
            pass
        return datetime.now().strftime("%Y%m%d")

    def set_date_value(self, ymd: str):
        if re.fullmatch(r"\d{8}", ymd):
            if hasattr(self.date_widget, "delete"): self.date_widget.delete(0, tk.END)
            if hasattr(self.date_widget, "insert"): self.date_widget.insert(0, ymd)
        else:
            messagebox.showwarning("æ—¥ä»˜å½¢å¼ã‚¨ãƒ©ãƒ¼", "YYYYMMDD ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    def run_fetch(self):
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢
        self.debug_text.config(state="normal")
        self.debug_text.delete(1.0, tk.END)
        self.debug_text.config(state="disabled")
        
        mode = self.mode.get()
        url_manual = self.ent_url.get().strip()

        if mode == "url" and url_manual:
            self._start_job(url_manual, soup=None)
            return

        ymd = self._current_ymd_or_today()
        place = self.cmb_place.get().strip()
        race = self.cmb_race.get().strip()

        if not (re.fullmatch(r"\d{8}", ymd) and place in VENUES and re.fullmatch(r"\d{1,2}R", race)):
            messagebox.showwarning("å…¥åŠ›å€¤ã‚¨ãƒ©ãƒ¼", "æ—¥ä»˜ã¯ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã§é¸æŠã€å ´æ‰€ã¯ãƒªã‚¹ãƒˆã‹ã‚‰ã€ãƒ¬ãƒ¼ã‚¹ã¯1Rã€œ12Rã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            return

        rno = int(race[:-1])
        self.btn.config(state="disabled"); self.status.config(text="3ã¤ã®æˆ¦ç•¥ã§æ¢ç´¢ä¸­...")
        self.debug_log(f"æ¢ç´¢é–‹å§‹: {ymd} {place} {rno}R")
        self.debug_log("æˆ¦ç•¥1â†’æˆ¦ç•¥2â†’æˆ¦ç•¥3ã®é †ã§è©¦è¡Œã—ã¾ã™\n")
        threading.Thread(target=self._auto_and_fetch, args=(ymd, place, rno), daemon=True).start()

    def _auto_and_fetch(self, ymd, place, rno):
        try:
            url, soup = build_jra_url_and_soup(
                ymd, place, rno, 
                status_cb=lambda s: self.after(0, lambda: self.status.config(text=s)),
                debug_log=self.debug_log
            )
            if not url:
                self._done("è©²å½“ã™ã‚‹ãƒ¬ãƒ¼ã‚¹ã®URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n\nã€æ¨å¥¨ã€‘JRAå…¬å¼ã‚µã‚¤ãƒˆã§è©²å½“ãƒ¬ãƒ¼ã‚¹ã®URLã‚’ã‚³ãƒ”ãƒ¼ã—ã€\nä¸Šéƒ¨ã®ã€ŒURLï¼ˆä»»æ„ï¼‰ã€æ¬„ã«è²¼ã‚Šä»˜ã‘ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                return
            rows, filename, race_title, used_url = fetch_rows_and_meta(url, soup)
            out = save_to_desktop(rows, filename, race_title)
            self._done(f"ä¿å­˜å®Œäº†ï¼š{out}\n\nä½¿ç”¨URLï¼ˆæ¬¡å›ç”¨ã«ãƒ¡ãƒ¢æ¨å¥¨ï¼‰ï¼š\n{used_url}")
        except Exception as e:
            self._done(f"ã‚¨ãƒ©ãƒ¼ï¼š{e}")

    def _start_job(self, url, soup=None):
        self.btn.config(state="disabled"); self.status.config(text="å–å¾—ä¸­â€¦")
        self.debug_log(f"URLç›´æ¥å–å¾—: {url}")
        threading.Thread(target=self._do_fetch, args=(url, soup), daemon=True).start()

    def _do_fetch(self, url, soup):
        try:
            rows, filename, race_title, used_url = fetch_rows_and_meta(url, soup)
            out = save_to_desktop(rows, filename, race_title)
            self._done(f"ä¿å­˜å®Œäº†ï¼š{out}\nä½¿ç”¨URLï¼š{used_url}")
        except Exception as e:
            self._done(f"ã‚¨ãƒ©ãƒ¼ï¼š{e}")

    def _done(self, msg):
        self.after(0, lambda: self._finish_ui(msg))

    def _finish_ui(self, msg):
        self.btn.config(state="normal"); self.status.config(text=msg.splitlines()[0])
        self.debug_log(f"\n{'='*60}\n{msg}\n{'='*60}")
        (messagebox.showinfo if msg.startswith("ä¿å­˜å®Œäº†") else messagebox.showwarning)("çµæœ", msg)

# ---------------- main ----------------
if __name__ == "__main__":
    App().mainloop()